{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESG Stock Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to build simple classification models to predict whether the U.S. stock will be added to the ESG portfolio. It is a binary classification problem where each asset has a target variable with a value of one, meaning that the stock is added to the ESG portfolio with the overall ESG score more than or equal to five and zero otherwise. \n",
    "\n",
    "This project is a simulation to help students understand the machine learning process widely used in the financial industries to help select the asset. The dataset contains  746 U.S. stocks that were preprocessed and merged from two sources:  \n",
    "1. https://www.kaggle.com/datasets/finintelligence/nasdaq-financial-fundamentals \n",
    "2. https://www.kaggle.com/datasets/debashish311601/esg-scores-and-ratings?resource=download\n",
    "\n",
    "\n",
    "The dataset contains outdated fundamental data and has not been entirely verified. Hence, using this dataset for personal academic assignments is not recommended. The information is not intended as financial advice and shall not be understood or construed as financial advice.\n",
    "\n",
    "The process are inspired by the paper 'Heterogeneous Ensemble for ESG Ratings Prediction' by Krappel, Boggun and Borth (2021). They collected fundamental data and built ML model to predict the ESG score. https://arxiv.org/abs/2109.10085"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook will outline the following processes: \n",
    "    \n",
    "1. Import the data\n",
    "2. Data Analysis\n",
    "3. Basic Data Transformation\n",
    "4. Prepare Data for Machine Learning Model\n",
    "5. Basic Machine Learning Models and Evaluate Performance\n",
    "6. Key takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library that we will use for this project\n",
    "# Pandas is popular Python library used in data analysis and manipulation https://pandas.pydata.org/\n",
    "# Numpy is another library for working on arrays and matrices https://numpy.org/\n",
    "# You will see more usecases of Pandas and Numpy in the next semester.\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variable called FILE_NAME in capital letters \n",
    "\n",
    "# Usually we declare in capital letters to seprate them from other variables \n",
    "# to let reader know that we do not want to reassign the value to this variable.\n",
    "# However, this approach does not actually prevent reassignment. \n",
    "\n",
    "FILE_NAME = 'US_Stock_ESG_and_Fundamental_seminar.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file to create DataFrame\n",
    "# DataFrame is two-dimensional data strcutures that has columns and rows.\n",
    "\n",
    "df = pd.read_csv(<>, index_col=0) #use the first column as the index\n",
    "df.<> #show the first 5 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the size of dataset (number of rows, number of columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving a summary of the dataframe\n",
    "# including the index dtype and columns, non-null values and memory usage.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many built-in data types available in Python. You can check from this site: https://www.w3schools.com/python/python_datatypes.asp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate descriptive statistics for numeric series\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate descriptive statistics for object (i.e., non-numeric) series\n",
    "df.describe(exclude=[np.number])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns in DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f you check the result from df.info(), some columns have objects as the data type instead of the numerical type such as float or integer (int)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the first row of column 'Assets'\n",
    "# we can see that type(var) gives Strinng data types\n",
    "\n",
    "print(df.Assets[0], <>(df.Assets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want these values to be numerical data i.e., int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that help us to convert values in the column\n",
    "\n",
    "def convert_to_numerical(row):\n",
    "    if isinstance(row, float): #if the data is None or NaN i.e., missing data\n",
    "        return 0\n",
    "    else:\n",
    "        return int(row.replace(',','')) #replace comma symbol and convert string value to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using lambda to apply function accorss all rows \n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "\n",
    "df['Assets'] = df['Assets'].apply(lambda x: convert_to_numerical(x))\n",
    "df['Cash and Cash Equivalents, at Carrying Value'] = df['Cash and Cash Equivalents, at Carrying Value'].apply(lambda x: convert_to_numerical(x))\n",
    "df['Final Revenue'] = df['Final Revenue'].apply(lambda x: convert_to_numerical(x))\n",
    "df['Gross Profit'] = df['Gross Profit'].apply(lambda x: convert_to_numerical(x))\n",
    "df['Income from Continuing Operations before Taxes'] = df['Income from Continuing Operations before Taxes'].apply(lambda x: convert_to_numerical(x))\n",
    "df['Operating Income (Loss)'] = df['Operating Income (Loss)'].apply(lambda x: convert_to_numerical(x))\n",
    "df['Total Equity'] = df['Total Equity'].apply(lambda x: convert_to_numerical(x))\n",
    "df['Total Liabilities and Equity'] = df['Total Liabilities and Equity'].apply(lambda x: convert_to_numerical(x))\n",
    "df['Net Income (Loss)'] = df['Net Income (Loss)'].apply(lambda x: convert_to_numerical(x))\n",
    "df['Cash and Cash Equivalents, Period Increase (Decrease)'] = df['Cash and Cash Equivalents, Period Increase (Decrease)'].apply(lambda x: convert_to_numerical(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run .describe() to check the data types again\n",
    "df.<>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(exclude=[np.number])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model could only take the numerical form. So we could not directly use a text of Country, Sector, Subsector in the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example values of Country, Sector and Subsector column\n",
    "# there are String \n",
    "print(df.Country[0], type(df.Country[0]))\n",
    "print(df.Sector[0], type(df.Sector[0]))\n",
    "print(df.Subsector[0], type(df.Subsector[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the technique called Label Encoding to convert the labels into a numeric form that the machine can read. https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Label encoding is available in the scikit-learn library, a ML library in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c intel scikit-learn\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels with value between 0 and n_classes-1. \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html \n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# crate new column as a result of LabelEncoder\n",
    "df['country_label'] = le.fit_transform(df['Country'])\n",
    "df['sector_label'] = le.fit_transform(df['Sector'])\n",
    "df['subsector_label'] = le.fit_transform(df['Subsector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['country_label', 'sector_label', 'subsector_label']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips: Another encoding method is one-hot encoding to handle Categorical data https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/?ref=lbp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Should we keep 'country_label' column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: No, all stocks are in the US and have the same 'US' value in Country column. There is no need to have this column to train ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a Series containing counts of unique values.\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html \n",
    "\n",
    "df['Country'].value_counts()\n",
    "# df.Country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unused columns -  Company Name, Country, Sector, Subsector\n",
    "dropped_columns = ['Ticker', 'Company Name', 'Country', 'country_label', 'Sector', 'Subsector']\n",
    "\n",
    "df.drop(dropped_columns, axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target variable vs Predictor variable\n",
    "\n",
    "- Target variable is the variable whose value is predicted by the model.\n",
    "- Predictor variable is the variable used to predict the target variable.\n",
    "\n",
    "In this project, the target variable is the 'Target' column in the DataFrame, which contains the binary value of zero and one. The value of one means that the stock has an overall ESG score greater than or equal to the median. The asset is added to the ESG portfolio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .value_counts() to see how many stocks are classified into zero and one value in Target column\n",
    "df.Target.<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data\n",
    "\n",
    "The next step is to split the dataset into training and testing dataset.\n",
    "- Training dataset is used to train and fit the ML model.\n",
    "- Testing data set is used to evaluate the performance of ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't need 'Target' column to be included in training dataset\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df.Target\n",
    "\n",
    "# train:test ratio is 80:20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The size of the training dataset: ', X_train.<>)\n",
    "print('The size of the testing dataset: ', X_test.<>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Target count of training dataset: ', y_train.value_counts().to_dict())\n",
    "print('Target count of testing dataset: ', y_test.<>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use three basic machine learning models.\n",
    "\n",
    "1. Logistics Regression\n",
    "2. Decision Tree\n",
    "3. Random Forest\n",
    "\n",
    "These supervised learning models require training datasets to learn and predict the value. They are commonly used for classification problems. Some models, such as decision tree and random forest, could be used to predict a continuous value (i.e., regression problem), such as predicting the house price.\n",
    "\n",
    "You will cover more details in the next semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree #decision tree\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many matrics that we can use to measure the performance of our prediction model. We will use accuracy, a fraction of how many predictions our model got right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def print_score(y_true, y_pred):\n",
    "    print('Accuracy: ', accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistics Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sigmoid function 𝑓(𝐱): 𝑝(𝐱) = 1 / (1 + exp(−𝑓(𝐱)) where 𝑓(𝐱) is a linear regression. The function can esimate probability that an instance belongs to a particular class. If the probaibility is greater than 50% then the model predicts that instance belongs to that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree is the model that divide instance into smaller decisions nodes and leaf. The model used the impurity function or loss function to split the decision node. The process continue spliting until the the impurity value is minimised. You can find mathematical formula on this website: https://scikit-learn.org/stable/modules/tree.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier().fit(<>)\n",
    "y_pred = clf.predict(<>)\n",
    "\n",
    "print_score(<>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an ensemble of decision trees  that fits a number of decision tree classifiers on various sub-samples of the dataset and trains via the bagging method. Random Forest uses averaging to improve the predictive accuracy and control over-fitting. You can read more on this article they have a clear example and visualisation https://towardsdatascience.com/understanding-random-forest-58381e0602d2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier().fit(<>)\n",
    "y_pred = clf.predict(<>)\n",
    "\n",
    "print_score(<>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Given the accuracy, which model is the best one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key takeaway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, you have learned the following:\n",
    "\n",
    "- How the industry has used a combination of the data to make a stock selection.\n",
    "- The nature of the structured dataset that could be processed in the form of DataFrame.\n",
    "- Basic data processing using Python programming language.\n",
    "- Basic machine learning models using Python and the scikit-learn library.\n",
    "- Basic machine learning performance evaluation using Accuracy as the main matric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is future work that you are encouraged to investigate further if you are interested in this topic:\n",
    "\n",
    "- How could you explain the source of E, S, G and overall ESG score? There is a lack of transparency in ESG ratings, and we do not know how the agency generates these ratings.\n",
    "- Further transformation of data. You can create a new column of financial ratios using the fundamental data.\n",
    "- This model hasn't been validated and doesn't have optimal hyperparameters. You could split the dataset into a validation dataset to train the model and find hyperparameters.\n",
    "- You can use other performance metrics, such as F1 score, recall and precision.\n",
    "- There are other classification models such as Naive Bayes, support vector machine, and advanced deep learning models.\n",
    "- You could find more data to improve the prediction result, such as sentiment from news and other ESG rating sources. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
